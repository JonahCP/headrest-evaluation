{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from turtle import title\n",
    "import mne\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, read_csv\n",
    "import numpy\n",
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from autoreject import get_rejection_threshold\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def format_events_frame(EventsDataFrame):\n",
    "    # Drop the first column since it's the same as the index\n",
    "    EventsDataFrame = EventsDataFrame.drop(EventsDataFrame.columns[0], axis=1)\n",
    "\n",
    "    # Convert 'datetimes' column to datetime format\n",
    "    EventsDataFrame['datetimes'] = pd.to_datetime(EventsDataFrame['datetimes'], format='%Y-%m-%d %H.%M.%S.%f',\n",
    "                                                  errors='coerce', utc=True)\n",
    "\n",
    "    # Gets the seconds that have passed since the trial started\n",
    "    EventsDataFrame['seconds_since_start'] = (\n",
    "            EventsDataFrame['datetimes'] - EventsDataFrame['datetimes'].min()).dt.total_seconds()\n",
    "\n",
    "    # Create 'TID' column based on conditions\n",
    "    conditions = [\n",
    "        (EventsDataFrame['events'] == 'START') | (EventsDataFrame['events'] == 'END'),\n",
    "        (EventsDataFrame['events'] == 'STANDARD'),\n",
    "        (EventsDataFrame['events'] == 'TARGET')\n",
    "    ]\n",
    "\n",
    "    values = [1, 10, 20]\n",
    "\n",
    "    # Adds the TID next to the events\n",
    "    EventsDataFrame['TID'] = np.select(conditions, values, default=np.nan)\n",
    "\n",
    "    # Reorder columns\n",
    "    EventsDataFrame = EventsDataFrame[\n",
    "        ['datetimes', 'seconds_since_start', 'events', 'TID'] + [col for col in EventsDataFrame.columns if\n",
    "                                                                 col not in ['datetimes', 'seconds_since_start',\n",
    "                                                                             'events', 'TID']]]\n",
    "\n",
    "    return EventsDataFrame\n",
    "\n",
    "\n",
    "def format_raw_data_frame(DataFrame, startTime):\n",
    "    # Convert 'datetime' column to datetime format\n",
    "    DataFrame['datetime'] = pd.to_datetime(DataFrame['datetime'], format='%Y-%m-%d %H.%M.%S.%f', errors='coerce',\n",
    "                                           utc=True)\n",
    "\n",
    "    # Gets the seconds that have passed since the trial started\n",
    "    DataFrame['seconds_since_start'] = (\n",
    "            DataFrame['datetime'] - startTime).dt.total_seconds()\n",
    "\n",
    "    # Get raw channel data\n",
    "    raw_channel_data = DataFrame[['chnl-1-raw', 'chnl-2-raw', 'chnl-3-raw', 'datetime', 'seconds_since_start']].values.T\n",
    "    # raw_channel_data = DataFrame[['chnl-1-delta', 'chnl-2-delta', 'chnl-3-delta', 'datetime', 'seconds_since_start']].values.T\n",
    "    # raw_channel_data = DataFrame[['chnl-1-theta', 'chnl-2-theta', 'chnl-3-theta', 'datetime', 'seconds_since_start']].values.T\n",
    "    # raw_channel_data = DataFrame[['chnl-1-alpha', 'chnl-2-alpha', 'chnl-3-alpha', 'datetime', 'seconds_since_start']].values.T\n",
    "    # raw_channel_data = DataFrame[['chnl-1-beta1', 'chnl-2-beta1', 'chnl-3-beta1', 'datetime', 'seconds_since_start']].values.T\n",
    "    # raw_channel_data = DataFrame[['chnl-1-beta2', 'chnl-2-beta2', 'chnl-3-beta2', 'datetime', 'seconds_since_start']].values.T\n",
    "\n",
    "    return raw_channel_data\n",
    "\n",
    "\n",
    "def numeric_participant_sort(dir):\n",
    "    return int(dir.lstrip(\"participant\"))\n",
    "\n",
    "\n",
    "def get_trimmed_files():\n",
    "    base_directory = 'hr_data'\n",
    "\n",
    "    # The line below searches the hr_data directory for other directories (which contain the test data)\n",
    "    directories = sorted([f for f in os.listdir(base_directory) if os.path.isdir(os.path.join(base_directory, f))],\n",
    "                         key=numeric_participant_sort)\n",
    "\n",
    "    files = []\n",
    "\n",
    "    for directory in directories:\n",
    "        for i in range(1, 9):\n",
    "            # file_path = f'{base_directory}/{directory}/erp{i}_trim.csv'\n",
    "            file_path = f'{base_directory}/{directory}/erp{i}_trim.csv'\n",
    "            files.append(file_path)\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_timestamps():\n",
    "    base_directory = 'hr_data'\n",
    "    # The line below searches the hr_data directory for other directories (which contain the test data)\n",
    "    directories = sorted([f for f in os.listdir(base_directory) if os.path.isdir(os.path.join(base_directory, f))],\n",
    "                         key=numeric_participant_sort)\n",
    "\n",
    "    files = []\n",
    "\n",
    "    for directory in directories:\n",
    "        subject_base_directory = f'{base_directory}/{directory}'\n",
    "        # Does the same as before except only grabs the erp directories for a single subject\n",
    "        erp_directories = [f for f in os.listdir(subject_base_directory)\n",
    "                           if os.path.isdir(os.path.join(subject_base_directory, f)) and 'erp' in f]\n",
    "\n",
    "        for erp_directory in erp_directories:\n",
    "            erp_directory_path = f'{base_directory}/{directory}/{erp_directory}'\n",
    "            erp_timestamp = [f for f in os.listdir(erp_directory_path) if re.match(r'erp_timestamps_.*\\.csv', f)][0]\n",
    "\n",
    "            files.append(f'{base_directory}/{directory}/{erp_directory}/{erp_timestamp}')\n",
    "\n",
    "    return files\n",
    "\n",
    "# last value will be the average\n",
    "def get_reaction_times():\n",
    "    import re\n",
    "    pattern = r\"\\d+\\.\\d+\"\n",
    "    milliseconds = []\n",
    "    with open(\"hr_data/participant_reaction_times.txt\", \"r\") as file:\n",
    "        for line in file:\n",
    "            matches = re.findall(pattern, line)\n",
    "            if matches:\n",
    "                milliseconds.extend(matches)\n",
    "\n",
    "    milliseconds = [float(ms) for ms in milliseconds]\n",
    "    return milliseconds\n",
    "\n",
    "def compute_mav_and_var_features(window_data, WSize=20, Olap=10):\n",
    "    mav_features = []\n",
    "    var_features = []\n",
    "\n",
    "    # # Calculate MAV and VAR from 250 ms to 550 ms\n",
    "    # for i in range(112, 172, Olap):\n",
    "    #     window_data = channel_2_data[:, i:i + WSize]\n",
    "\n",
    "    #     # Calculate MAV\n",
    "    #     mav = np.mean(np.abs(window_data), axis=1)\n",
    "    #     mav_features.append(mav)\n",
    "\n",
    "    #     # Calculate VAR\n",
    "    #     # Newaxis is used to match the shape of the two arrays\n",
    "    #     var = 1/WSize * np.sum(np.square(window_data - mav[:, np.newaxis]), axis=1)\n",
    "    #     var_features.append(var)\n",
    "\n",
    "    indices = [int(207 * .370 +52), int(207 * .490 + 52)]\n",
    "    for i in indices:\n",
    "        window_data = channel_2_data[:, i:i + 30]\n",
    "\n",
    "        # Calculate MAV\n",
    "        mav = np.mean(np.abs(window_data), axis=1)\n",
    "        mav_features.append(mav)\n",
    "\n",
    "        # Calculate VAR\n",
    "        # Newaxis is used to match the shape of the two arrays\n",
    "        var = 1/30 * np.sum(np.square(window_data - mav[:, np.newaxis]), axis=1)\n",
    "        var_features.append(var)\n",
    "\n",
    "    return np.array(mav_features), np.array(var_features)\n",
    "\n",
    "TrimmedERPFiles = get_trimmed_files()\n",
    "ERPTimestamps = get_timestamps()\n",
    "\n",
    "FileMatrix = [TrimmedERPFiles, ERPTimestamps]\n",
    "\n",
    "FileDataFrame = pd.DataFrame(FileMatrix)\n",
    "\n",
    "# Initialize an empty list to store ERP data for each channel\n",
    "list_of_avg_base_stimuli_across_all_trials = []\n",
    "list_of_avg_target_stimuli_across_all_trials = []\n",
    "\n",
    "num_of_targets = 0\n",
    "num_of_base = 0\n",
    "drop_count_target = 0\n",
    "drop_count_base = 0\n",
    "\n",
    "classifier_data = []\n",
    "\n",
    "for i in range(FileDataFrame.columns.size):\n",
    "    ERPDataFrame = read_csv(FileDataFrame[i].iloc[0])\n",
    "    ERPEventsDataFrame = read_csv(FileDataFrame[i].iloc[1])\n",
    "\n",
    "    events_info = format_events_frame(ERPEventsDataFrame)\n",
    "\n",
    "    # start time of the trial\n",
    "    start_time = events_info['datetimes'].iloc[0]\n",
    "\n",
    "    # Pass in the ERPDataFrame and the start time of the trial to have an accurate start time for both\n",
    "    raw_array = format_raw_data_frame(ERPDataFrame, start_time)\n",
    "\n",
    "    ch_names = ['Channel 1', 'Channel 2', 'Channel 3']\n",
    "\n",
    "    ch_types = ['eeg', 'eeg', 'eeg']\n",
    "\n",
    "    sampling_frequency = 206\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, ch_types=ch_types, sfreq=sampling_frequency)\n",
    "\n",
    "    raw = mne.io.RawArray(raw_array[:3, :], info).set_meas_date(start_time)\n",
    "\n",
    "    onset_array = events_info['seconds_since_start'].values\n",
    "\n",
    "    description_array = events_info['TID'].values.astype(int)\n",
    "\n",
    "    annotations = mne.Annotations(onset=onset_array, description=description_array, duration=0.001953125,\n",
    "                                  orig_time=start_time)\n",
    "\n",
    "    raw.set_annotations(annotations)\n",
    "\n",
    "    # Begin analysis\n",
    "    raw.pick_channels(ch_names)\n",
    "\n",
    "    # raw.plot(scalings='auto')\n",
    "\n",
    "    # raw.compute_psd().plot()\n",
    "\n",
    "    # Use 5 to 11 to capture Theta + Alpha bands\n",
    "    # raw.filter(5, 11, fir_design='firwin', fir_window='hamming')\n",
    "    # raw.filter(5, 7, fir_design='firwin', fir_window='hamming')\n",
    "    raw.filter(5, 27, fir_design='firwin', fir_window='hamming')\n",
    "\n",
    "    # raw.compute_psd().plot()\n",
    "\n",
    "    # Define events based on your experimental paradigm\n",
    "    events, event_id = mne.events_from_annotations(raw)\n",
    "\n",
    "    # Where 2 is base and 3 is target stimuli\n",
    "    if 31 < i < 48 or i == 64:\n",
    "        event_ids_of_interest = [2, 3]\n",
    "    else:\n",
    "        # Where 3 is base and 4 is target stimuli\n",
    "        event_ids_of_interest = [3, 4]\n",
    "        # event_ids_of_interest = [2, 3]\n",
    "\n",
    "    # Picks are just defining what type of channels we are using, in this case EEG and EOG\n",
    "    picks = mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "    # Create epochs around events of interest (e.g., visual stimuli)\n",
    "    base_epochs = None\n",
    "    target_epochs = None\n",
    "\n",
    "    # Define rejection thresholds for each channel\n",
    "    # rejection_thresholds = {'Channel 1': 5000, 'Channel 2': 8000, 'Channel 3': 8000}\n",
    "\n",
    "    for eid in event_ids_of_interest:\n",
    "        final_epochs = mne.Epochs(raw, events, eid, tmin=-0.25, tmax=0.75,\n",
    "                                  preload=True, event_repeated='merge')\n",
    "\n",
    "        reject = get_rejection_threshold(final_epochs)['eeg']\n",
    "\n",
    "        final_epochs.drop_bad(reject={'eeg': reject})\n",
    "\n",
    "        # This line throws out the data for the participant clicking the space bar on all shapes\n",
    "        if i == 72:\n",
    "            final_epochs.drop_bad(reject={'eeg': 0})\n",
    "\n",
    "        # Get drop log\n",
    "        drop_log = final_epochs.drop_log\n",
    "\n",
    "        # count dropped epochs\n",
    "        # Loop through each channel name\n",
    "\n",
    "        # Plot all epochs\n",
    "        # final_epochs.plot(scalings='auto', n_epochs=5)  # Plot a subset of epochs for visualization\n",
    "\n",
    "        # Plot the dropped epochs\n",
    "        # for i, d in enumerate(drop_log):\n",
    "        #     if d and eid == 4:\n",
    "        #         if d[0] != \"IGNORED\":\n",
    "        #             drop_count += 1\n",
    "                    # print(f\"Dropped epochs for event index {i}:\")\n",
    "        #             # final_epochs[4].plot(scalings='auto')\n",
    "\n",
    "        # final_epochs.plot(scalings='auto')\n",
    "\n",
    "        # final_epochs.plot(picks=picks, events=events, scalings=scalings)\n",
    "        if final_epochs.__len__() != 0:\n",
    "            final_epochs = final_epochs.apply_baseline(baseline=(-0.25, 0))\n",
    "\n",
    "            if eid == event_ids_of_interest[0]:\n",
    "                base_epochs = final_epochs.copy()\n",
    "                num_of_base += base_epochs.events.shape[0]\n",
    "                drop_count_base += 80 - final_epochs.__len__()\n",
    "            else:\n",
    "                # final_epochs.plot(scalings='auto')\n",
    "                target_epochs = final_epochs.copy()\n",
    "                num_of_targets += target_epochs.events.shape[0]\n",
    "                drop_count_target += 20 - final_epochs.__len__()\n",
    "\n",
    "    # base_epochs.plot(picks=picks, events=events, scalings='auto')\n",
    "    # target_epochs.plot(picks=picks, events=events, scalings='auto')\n",
    "\n",
    "    # The following are evoked objects in mne\n",
    "    if base_epochs is not None:\n",
    "        base_epochs_avg = base_epochs.average(picks='eeg')\n",
    "        list_of_avg_base_stimuli_across_all_trials.append(base_epochs_avg)\n",
    "\n",
    "        # Calculate mean average value (MAV) with WSize = 20 and Olap = 10 for second channel\n",
    "        channel_2_data = base_epochs.get_data(picks='eeg')[:, 1, :]\n",
    "\n",
    "        # Calculate MAV and VAR features\n",
    "        mav_features, var_features = compute_mav_and_var_features(channel_2_data)\n",
    "\n",
    "        # Feature dictionary\n",
    "        for j in range(mav_features.shape[1]):\n",
    "            feature_dict = {\n",
    "                'Participant': i,\n",
    "                'Base/Target': 0\n",
    "            }\n",
    "            for k in range(mav_features.shape[0]):\n",
    "                feature_dict[f'MAV_{k}'] = mav_features[k, j]\n",
    "                feature_dict[f'VAR_{k}'] = var_features[k, j]\n",
    "            classifier_data.append(feature_dict)\n",
    "\n",
    "    if target_epochs is not None:\n",
    "        target_epochs_avg = target_epochs.average(picks='eeg')\n",
    "        list_of_avg_target_stimuli_across_all_trials.append(target_epochs_avg)\n",
    "\n",
    "        # Calculate mean average value (MAV) with WSize = 20 and Olap = 10 for second channel\n",
    "        channel_2_data = target_epochs.get_data(picks='eeg')[:, 1, :]\n",
    "\n",
    "        # Calculate MAV and VAR features\n",
    "        mav_features, var_features = compute_mav_and_var_features(channel_2_data)\n",
    "\n",
    "        # Feature dictionary\n",
    "        for j in range(mav_features.shape[1]):\n",
    "            feature_dict = {\n",
    "                'Participant': i,\n",
    "                'Base/Target': 1\n",
    "            }\n",
    "            for k in range(mav_features.shape[0]):\n",
    "                feature_dict[f'MAV_{k}'] = mav_features[k, j]\n",
    "                feature_dict[f'VAR_{k}'] = var_features[k, j]\n",
    "            classifier_data.append(feature_dict)\n",
    "\n",
    "classifier_df = pd.DataFrame(classifier_data)\n",
    "\n",
    "# Balance dataset for classification\n",
    "base_df = classifier_df[classifier_df['Base/Target'] == 0]\n",
    "target_df = classifier_df[classifier_df['Base/Target'] == 1]\n",
    "\n",
    "sampled_base_df = base_df.sample(n=target_df.shape[0], random_state=42)\n",
    "\n",
    "balanced_df = pd.concat([sampled_base_df, target_df])\n",
    "\n",
    "# base_epochs_avg.plot(picks='eeg')\n",
    "# target_epochs_avg.plot(picks='eeg')\n",
    "# Take the grand average of the base and target stimulis\n",
    "grand_average_base_stimuli = mne.grand_average(list_of_avg_base_stimuli_across_all_trials)\n",
    "grand_average_target_stimuli = mne.grand_average(list_of_avg_target_stimuli_across_all_trials)\n",
    "\n",
    "# Calculate the number of rows and columns for the grid\n",
    "num_rows = 3\n",
    "num_cols = 1\n",
    "\n",
    "# Calculate the number of subplots needed\n",
    "num_subplots = int(np.ceil(len(ch_names) / (num_rows * num_cols)))\n",
    "\n",
    "# Determine y-axis limits across all channels\n",
    "min_y = float('inf')\n",
    "max_y = float('-inf')\n",
    "\n",
    "for ch_name in ch_names:\n",
    "    base_data = grand_average_base_stimuli.get_data(picks=ch_name)\n",
    "    target_data = grand_average_target_stimuli.get_data(picks=ch_name)\n",
    "\n",
    "    min_y = min(min_y, np.min([base_data, target_data]))\n",
    "    max_y = max(max_y, np.max([base_data, target_data]))\n",
    "\n",
    "\n",
    "# Get reaction times and standard deviation\n",
    "reaction_times = get_reaction_times()\n",
    "std_dev = np.std(reaction_times[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaction Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over subplots\n",
    "for subplot_idx in range(num_subplots):\n",
    "    start_channel_idx = subplot_idx * num_rows * num_cols\n",
    "    end_channel_idx = (subplot_idx + 1) * num_rows * num_cols\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 12))\n",
    "\n",
    "    # Flatten the axes array for easy indexing\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop over channels and plot on separate subplots\n",
    "    for idx, ch_name in enumerate(ch_names[start_channel_idx:end_channel_idx]):\n",
    "        base_data = grand_average_base_stimuli.get_data(picks=ch_name)\n",
    "        target_data = grand_average_target_stimuli.get_data(picks=ch_name)\n",
    "\n",
    "        # Plot the data for the current channel on the corresponding subplot\n",
    "        axes[idx].plot(grand_average_base_stimuli.times * 1000, base_data[0], label='Base Stimuli', color='blue')\n",
    "        axes[idx].plot(grand_average_target_stimuli.times * 1000, target_data[0], label='Target Stimuli', color='red')\n",
    "\n",
    "        # Add vertical lines at specific time points\n",
    "        axes[idx].axvline(x=300, color='green', linestyle='--', label='Vertical Line at 300ms')\n",
    "        axes[idx].axvline(x=reaction_times[-1], color='orange', linestyle='--', label='Avg Reaction Time')\n",
    "\n",
    "        # Plot histogram of reaction times around the average reaction time\n",
    "        bins = np.linspace(reaction_times[-1] - 2 * std_dev, reaction_times[-1] + 2 * std_dev, 20)\n",
    "        axes[idx].hist(reaction_times, bins=bins, alpha=0.5, color='gray', label='Reaction Time Distribution')\n",
    "\n",
    "        # Add a horizontal line at y=0\n",
    "        axes[idx].axhline(y=0, color='black', linestyle='-', linewidth=1, label='Zero Line')\n",
    "\n",
    "        # Set labels and title\n",
    "        axes[idx].set_title(f'Grand Averaged Headrest Epochs for {ch_name}')\n",
    "        axes[idx].set_xlabel('Time (ms)')\n",
    "        axes[idx].set_ylabel('Amplitude (uV)')\n",
    "\n",
    "        # Invert the y-axis to flip the negative values upwards\n",
    "        # axes[idx].invert_yaxis()\n",
    "\n",
    "        # Set y-axis limits\n",
    "        axes[idx].set_ylim(min_y, max_y)\n",
    "\n",
    "        # Set x-axis limits to 0 to 600 milliseconds\n",
    "        axes[idx].set_xlim(0, 600)\n",
    "\n",
    "        # Add legend\n",
    "        axes[idx].legend(loc='lower left', fontsize=8)\n",
    "\n",
    "        # Add annotations for Targets Shown and Bases Dropped\n",
    "        targets_shown = f\"Targets Shown: {num_of_targets}\"\n",
    "        targets_dropped = f\"Targets Dropped: {drop_count_target}\"\n",
    "        bases_shown = f\"Bases Shown: {num_of_base}\"\n",
    "        bases_dropped = f\"Bases Dropped: {drop_count_base}\"\n",
    "        axes[idx].annotate(bases_shown, xy=(0.02, 0.92), xycoords='axes fraction', fontsize=8, color='blue')\n",
    "        axes[idx].annotate(bases_dropped, xy=(0.02, 0.86), xycoords='axes fraction', fontsize=8, color='blue')\n",
    "        axes[idx].annotate(targets_shown, xy=(0.02, 0.80), xycoords='axes fraction', fontsize=8, color='red')\n",
    "        axes[idx].annotate(targets_dropped, xy=(0.02, 0.74), xycoords='axes fraction', fontsize=8, color='red')\n",
    "\n",
    "    # Adjust layout with auto spacing\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph with p-values \n",
    "\n",
    "# Convert data to numpy arrays\n",
    "data_base = np.array([evoked.data for evoked in list_of_avg_base_stimuli_across_all_trials])\n",
    "data_target = np.array([evoked.data for evoked in list_of_avg_target_stimuli_across_all_trials])\n",
    "\n",
    "# Perform paired t-test\n",
    "p_values = []\n",
    "for i in range(data_base.shape[2]):\n",
    "    _, p = ttest_rel(data_base[:, :, i], data_target[:, :, i], axis=0)\n",
    "    p_values.append(p)\n",
    "\n",
    "p_values = np.array(p_values).T\n",
    "\n",
    "# Loop over subplots and show shading of p-values below 0.05\n",
    "for subplot_idx in range(num_subplots):\n",
    "    start_channel_idx = subplot_idx * num_rows * num_cols\n",
    "    end_channel_idx = (subplot_idx + 1) * num_rows * num_cols\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 12))\n",
    "\n",
    "    # Flatten the axes array for easy indexing\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Loop over channels and plot on separate subplots\n",
    "    for idx, ch_name in enumerate(ch_names[start_channel_idx:end_channel_idx]):\n",
    "        base_data = grand_average_base_stimuli.get_data(picks=ch_name)\n",
    "        target_data = grand_average_target_stimuli.get_data(picks=ch_name)\n",
    "\n",
    "        # Plot the data for the current channel on the corresponding subplot\n",
    "        axes[idx].plot(grand_average_base_stimuli.times * 1000, base_data[0], label='Base Stimuli', color='blue')\n",
    "        axes[idx].plot(grand_average_target_stimuli.times * 1000, target_data[0], label='Target Stimuli', color='red')\n",
    "\n",
    "        # Add vertical lines at specific time points\n",
    "        axes[idx].axvline(x=300, color='green', linestyle='--', label='Vertical Line at 300ms')\n",
    "        axes[idx].axvline(x=reaction_times[-1], color='orange', linestyle='--', label='Avg Reaction Time')\n",
    "\n",
    "        # Add a horizontal line at y=0\n",
    "        axes[idx].axhline(y=0, color='black', linestyle='-', linewidth=1, label='Zero Line')\n",
    "\n",
    "        # Set labels and title\n",
    "        axes[idx].set_title(f'Grand Averaged Headrest Epochs for {ch_name}')\n",
    "        axes[idx].set_xlabel('Time (ms)')\n",
    "        axes[idx].set_ylabel('Amplitude (uV)')\n",
    "\n",
    "        # Set y-axis limits\n",
    "        axes[idx].set_ylim(min_y, max_y)\n",
    "\n",
    "        # Set x-axis limits to 0 to 600 milliseconds\n",
    "        axes[idx].set_xlim(0, 600)\n",
    "\n",
    "        # Shade the area where p-values are below 0.05\n",
    "        axes[idx].fill_between(grand_average_base_stimuli.times * 1000, min_y, max_y,\n",
    "                            where=p_values[idx] < 0.05, color='gray', alpha=0.5, label='p < 0.05')\n",
    "        \n",
    "        # Add legend\n",
    "        axes[idx].legend(loc='lower left', fontsize=8)\n",
    "\n",
    "        # Add annotations for Targets Shown and Bases Dropped\n",
    "        targets_shown = f\"Targets Shown: {num_of_targets}\"\n",
    "        targets_dropped = f\"Targets Dropped: {drop_count_target}\"\n",
    "        bases_shown = f\"Bases Shown: {num_of_base}\"\n",
    "        bases_dropped = f\"Bases Dropped: {drop_count_base}\"\n",
    "        axes[idx].annotate(bases_shown, xy=(0.02, 0.92), xycoords='axes fraction', fontsize=8, color='blue')\n",
    "        axes[idx].annotate(bases_dropped, xy=(0.02, 0.86), xycoords='axes fraction', fontsize=8, color='blue')\n",
    "        axes[idx].annotate(targets_shown, xy=(0.02, 0.80), xycoords='axes fraction', fontsize=8, color='red')\n",
    "        axes[idx].annotate(targets_dropped, xy=(0.02, 0.74), xycoords='axes fraction', fontsize=8, color='red')\n",
    "\n",
    "    # Adjust layout with auto spacing\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier to classify between base and target stimuli only using channel 2 \n",
    "X = balanced_df.drop(columns=['Participant', 'Base/Target'])\n",
    "y = balanced_df['Base/Target']\n",
    "groups = balanced_df['Participant']\n",
    "cv = LeaveOneGroupOut()\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Initialize lists to store the accuracy and confusion matrix\n",
    "accuracy = []\n",
    "aucs = []\n",
    "f1_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "for train_index, test_index in cv.split(X, y, groups):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy.append(clf.score(X_test, y_test))\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    aucs.append(auc)\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "mean_accuracy = np.mean(accuracy)\n",
    "mean_auc = np.mean(aucs)\n",
    "\n",
    "# Calculate the mean confusion matrix\n",
    "mean_cm = np.sum(confusion_matrices, axis=0) \n",
    "macro_f1 = np.mean(f1_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(mean_cm, annot=True, cmap='Blues', xticklabels=['Base', 'Target'], yticklabels=['Base', 'Target'], fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title(f'Confusion Matrix\\nAccuracy: {mean_accuracy:.2f}\\nAUC: {mean_auc:.2f}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
